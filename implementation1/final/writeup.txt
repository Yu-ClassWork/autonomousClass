What I implemented for this paper was the example that the authors talked about within the paper. The example is the tiger problem, which looks at a POMDP where you want to maximize the reward the team gets. I created 3 different types of policies for my implementations. These 3 implementations are for looking at the different policies and how they act. The two of the policies give negative results, where the last policy (3) gave a positive reward. The first two policies had random actions from two and one agents respectively. The last policy both agents always listened unless they had a better understanding of the state space based on the set of observations. When running my program, since it is python, you type "python implementation1.py". Once you run the program you pick a policy between 1-3, which will then give you the rewards graph over 300 steps. The reward graphs will always be different for the first two policies because of the randomness, but the last policy will always give the same graph because of the policy that is implemented.